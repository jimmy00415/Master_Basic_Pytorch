{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2e8ecf",
   "metadata": {},
   "source": [
    "---\n",
    "## ReLU Activation Function\n",
    "\n",
    "### What is ReLU?\n",
    "\n",
    "**ReLU** (Rectified Linear Unit) is the most popular activation function in modern neural networks.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**In simple terms:**\n",
    "- If input > 0: output = input (pass through)\n",
    "- If input ≤ 0: output = 0 (zero out)\n",
    "\n",
    "**Advantages:**\n",
    "1. ✅ Computationally efficient (simple max operation)\n",
    "2. ✅ Helps mitigate vanishing gradient problem\n",
    "3. ✅ Introduces non-linearity (crucial for learning complex patterns)\n",
    "4. ✅ Sparse activation (many zeros)\n",
    "\n",
    "**Why non-linearity matters:**\n",
    "Without activation functions, stacking multiple linear layers would be equivalent to a single linear layer. Activation functions allow networks to learn complex, non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66348b",
   "metadata": {},
   "source": [
    "### Example 3.1: Basic ReLU on a Vector (Functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input tensor\n",
    "x = torch.tensor([-2.0, -0.5, 0.0, 0.5, 2.0])\n",
    "relu_output = F.relu(x)\n",
    "\n",
    "print(\"Input: \", x)\n",
    "print(\"ReLU Output: \", relu_output)\n",
    "print(\"\\n⚡ Notice: negative values → 0, positive values unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22098006",
   "metadata": {},
   "source": [
    "### Example 3.2: Basic ReLU on a Vector (Module API)\n",
    "\n",
    "**Two ways to use ReLU:**\n",
    "1. `F.relu(x)`: Functional API (for one-time use)\n",
    "2. `nn.ReLU()`: Module API (for use in `nn.Sequential` or custom modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80303d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-2.0, -0.5, 0.0, 0.5, 2.0])\n",
    "relu = nn.ReLU()\n",
    "\n",
    "output = relu(x)\n",
    "print(\"Input: \", x)\n",
    "print(\"After ReLU: \", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bbe73",
   "metadata": {},
   "source": [
    "### Example 3.3: ReLU on a Matrix\n",
    "\n",
    "ReLU operates **element-wise** on matrices (and tensors of any dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6272964",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[-1.0, 2.0], [0.0, -3.5]])\n",
    "output = nn.ReLU()(x)\n",
    "\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"\\nOutput:\\n\", output)\n",
    "print(\"\\n⚡ Element-wise: negatives→0, positives unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f038f85",
   "metadata": {},
   "source": [
    "### Example 3.4: ReLU in a Neural Network\n",
    "\n",
    "In practice, ReLU is placed between linear layers to introduce non-linearity.\n",
    "\n",
    "**Network flow:** Input → Linear(4→3) → ReLU → Linear(3→2) → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13306ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 2)\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 4)\n",
    "output = model(x)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a20427",
   "metadata": {},
   "source": [
    "### Example 3.5: Visualizing the ReLU Function\n",
    "\n",
    "This graph shows:\n",
    "- **Left side (x < 0)**: flat at 0\n",
    "- **Right side (x ≥ 0)**: diagonal line (identity)\n",
    "- **At x = 0**: there's a \"kink\" or bend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "y = F.relu(x)\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy(), linewidth=2)\n",
    "plt.title(\"ReLU Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
