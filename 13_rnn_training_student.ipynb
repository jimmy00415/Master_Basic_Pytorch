{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59230e6d",
      "metadata": {
        "id": "59230e6d"
      },
      "source": [
        "\n",
        "# ðŸŽ“ RNN Training 101\n",
        "\n",
        "You'll train a simple sequence model (RNN / LSTM / GRU) on a synthetic time series and **visualize loss curves and predictions**.\n",
        "\n",
        "**Learning goals**\n",
        "- Understand what **sequence data** is and how to build time-series samples with a sliding window: shape **(batch, seq_len, features)**.\n",
        "- Implement and compare **RNN / LSTM / GRU** forward passes and training loops.\n",
        "- Plot the **training loss** and **model predictions vs. ground truth**.\n",
        "- Experiment with hyperparameters and observe their effects.\n",
        "\n",
        "> How to use: Run each cell from top to bottom. Works out-of-the-box in Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dddf094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dddf094",
        "outputId": "3069851a-e636-4bb2-b9b9-b805ea9a1d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# âœ… Environment & Reproducibility\n",
        "import math, random, os, sys, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42\n",
        "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee74395e",
      "metadata": {
        "id": "ee74395e"
      },
      "source": [
        "\n",
        "## 1) What is **sequence data**?\n",
        "\n",
        "Sequence data is any data where **order matters** and each element may depend on previous elements. Common examples:\n",
        "- **Time series** (sensor readings, stock prices, weather).  \n",
        "- **Language** (characters/words in a sentence).  \n",
        "- **Audio** (waveform samples over time).  \n",
        "\n",
        "Key concepts you should know:\n",
        "- **Causality:** to predict the next point at time `t+1`, the model can only use information up to time `t`.\n",
        "- **Windowing:** we make training samples by taking a **fixed-length window** of past values (length = `seq_len`) as input `X`, and a **target** as either the **next** value (one-step ahead) or a **future segment** (multi-step).\n",
        "- **Shapes:** we usually arrange a mini-batch as `(batch, seq_len, features)`. For a univariate time series, `features=1`.\n",
        "- **Overlapping vs. non-overlapping windows:** overlapping gives more samples and smoother learning; non-overlapping reduces redundancy.\n",
        "- **Normalization:** standardize/scale values to stabilize training.\n",
        "- **Train/Validation split:** split along the **time axis** to avoid leaking future information into the past.\n",
        "- **Padding/Masking (advanced):** for variable-length sequences, we may pad to the same length and use masks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2e8396",
      "metadata": {
        "id": "ec2e8396"
      },
      "source": [
        "\n",
        "## 2) Build a simple time-series dataset (sine mix + noise)\n",
        "\n",
        "**Task:** given the past `seq_len` points, **predict the next point** (one-step ahead regression).  \n",
        "We create a long series by mixing several sine waves, add light noise, then cut it into many `(X, y)` samples using a **sliding window**.\n",
        "\n",
        "**Data pipeline overview**\n",
        "1. Generate a long normalized series `series[t]`.\n",
        "2. Split into **train** (earlier times) and **validation** (later times) to respect causality.\n",
        "3. Use a sliding window of length `seq_len` to build input `X` and target `y`:\n",
        "   - `X[i] = series[i : i+seq_len]`\n",
        "   - `y[i] = series[i+seq_len]` (the next value)\n",
        "4. Stack windows into tensors with shapes:\n",
        "   - `X`: `(N, seq_len, 1)`\n",
        "   - `y`: `(N, 1)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z2vUqJR6tb5b",
      "metadata": {
        "id": "Z2vUqJR6tb5b"
      },
      "source": [
        "\n",
        "### ðŸ” Example: Creating sequence samples step by step\n",
        "\n",
        "Let's start from a **simple numeric sequence**:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{series} = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "$$\n",
        "\n",
        "If we choose `seq_len = 3`,  \n",
        "each training input `X` is the last 3 numbers, and the target `y` is the **next** number.\n",
        "\n",
        "| Sample Index | Input Sequence `X` | Target `y` |\n",
        "| :----------- | :----------------- | :--------- |\n",
        "| 0            | [1, 2, 3]          | 4          |\n",
        "| 1            | [2, 3, 4]          | 5          |\n",
        "| 2            | [3, 4, 5]          | 6          |\n",
        "| 3            | [4, 5, 6]          | 7          |\n",
        "| 4            | [5, 6, 7]          | 8          |\n",
        "\n",
        "So our dataset becomes:\n",
        "- **X shape:** (5 samples, 3 time steps, 1 feature)\n",
        "- **y shape:** (5 samples, 1 value)\n",
        "\n",
        "This is exactly how we prepare inputs for RNNs â€” each row in `X` is a small **time window** used to predict the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c75c20d",
      "metadata": {
        "id": "0c75c20d"
      },
      "outputs": [],
      "source": [
        "# 2A) Dataset class\n",
        "class SineMixDataset(Dataset):\n",
        "    def __init__(self, total_len=6000, seq_len=30, train=True, train_ratio=0.8):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        t = np.linspace(0, 200, total_len).astype(np.float32)\n",
        "        # Mix multiple sine waves with different frequencies/phases + mild noise\n",
        "        x = (np.sin(0.3*t) + 0.5*np.sin(0.05*t + 1.0) + 0.3*np.sin(0.9*t + 0.5)).astype(np.float32)\n",
        "        x += 0.05 * np.random.randn(total_len).astype(np.float32)\n",
        "        # Normalize\n",
        "        x = (x - x.mean()) / (x.std() + 1e-6)\n",
        "        # Train/Val split along time\n",
        "        split = int(total_len * train_ratio)\n",
        "        if train:\n",
        "            self.series = x[:split]\n",
        "        else:\n",
        "            self.series = x[split:]\n",
        "        # Build (X, y) windows using a sliding window\n",
        "        X, y = [], []\n",
        "        for i in range(0, len(self.series)-seq_len-1):\n",
        "            X.append(self.series[i:i+seq_len])\n",
        "            y.append(self.series[i+seq_len])\n",
        "        self.X = np.array(X, dtype=np.float32)[:, :, None]   # (N, seq_len, 1)\n",
        "        self.y = np.array(y, dtype=np.float32)[:, None]      # (N, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.y[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UpAw6_etuqBk",
      "metadata": {
        "id": "UpAw6_etuqBk"
      },
      "source": [
        "# 2B) Quick sanity check\n",
        "\n",
        "The purpose of the â€œQuick sanity checkâ€ is to verify that the dataset is generated correctly, with proper shapes and data structure, and that it can be loaded by the DataLoader without errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac94e8f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac94e8f4",
        "outputId": "e6a548ef-7d71-41a6-cda0-abd69defdcdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 4769 | Val samples: 1169\n",
            "One sample X shape: torch.Size([30, 1]) y shape: torch.Size([1])\n",
            "Batch X shape: torch.Size([64, 30, 1]) | Batch y shape: torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "train_ds = SineMixDataset(train=True, seq_len=30)\n",
        "val_ds   = SineMixDataset(train=False, seq_len=30)\n",
        "\n",
        "print(\"Train samples:\", len(train_ds), \"| Val samples:\", len(val_ds))\n",
        "x0, y0 = train_ds[0]\n",
        "print(\"One sample X shape:\", x0.shape, \"y shape:\", y0.shape)  # (seq_len, 1), (1,)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "for xb, yb in train_loader:\n",
        "    print('Batch X shape:', xb.shape, '| Batch y shape:', yb.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mz8lgFoculpi",
      "metadata": {
        "id": "Mz8lgFoculpi"
      },
      "source": [
        "### 2D) Inspect a few samples from the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JpiPx_Bauk40",
      "metadata": {
        "id": "JpiPx_Bauk40"
      },
      "outputs": [],
      "source": [
        "# Pick one training sample and print its input and target\n",
        "sample_X, sample_y = train_ds[0]\n",
        "print(\"Sample 0 input (X) shape:\", sample_X.shape)\n",
        "print(\"Sample 0 target (y) shape:\", sample_y.shape)\n",
        "#print(\"First few values of X:\", sample_X.squeeze()[:31].numpy())\n",
        "#print(\"Target y value:\", float(sample_y))\n",
        "\n",
        "# Optional: visualize a portion of the training sequence and highlight first window\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(train_ds.series[:500], label='Training series')\n",
        "plt.axvspan(0, train_ds.seq_len, color='orange', alpha=0.2, label='Example input window')\n",
        "plt.axvline(train_ds.seq_len, color='red', linestyle='--', label='Target position')\n",
        "plt.title(\"Example: Sliding window on sine-mix data\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3wAYmYumxPfN",
      "metadata": {
        "id": "3wAYmYumxPfN"
      },
      "source": [
        "### 2E) Visualize one full sequence and its target point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93UgRrD_xOYx",
      "metadata": {
        "id": "93UgRrD_xOYx"
      },
      "outputs": [],
      "source": [
        "\n",
        "sample_index = 0\n",
        "sample_X, sample_y = train_ds[sample_index]\n",
        "\n",
        "# Print all 30 input values and the target\n",
        "print(f\"Sample {sample_index} input (X) shape: {sample_X.shape}\")\n",
        "print(f\"Sample {sample_index} target (y) shape: {sample_y.shape}\\n\")\n",
        "\n",
        "print(\"Full input sequence (30 steps):\")\n",
        "print(np.round(sample_X.squeeze().numpy(), 4))  # Rounded for readability\n",
        "print(f\"\\nTarget y (next value): {float(sample_y):.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(range(len(sample_X)), sample_X.squeeze().numpy(), 'b-o', label='Input sequence (X)')\n",
        "plt.plot(len(sample_X), float(sample_y), 'ro', label='Target (next step y)')\n",
        "plt.title(\"One complete sample: 30-step input and 1-step target\")\n",
        "plt.xlabel(\"Time step within the sample\")\n",
        "plt.ylabel(\"Signal value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9fd150",
      "metadata": {
        "id": "ac9fd150"
      },
      "source": [
        "\n",
        "## 3) Define a flexible RNN model (RNN / LSTM / GRU)\n",
        "\n",
        "### ðŸ•’ Step-by-Step Sequence Processing in LSTM\n",
        "\n",
        "Each sequence has shape **(B, T, F)**\n",
        "\n",
        "* **B** â€” batch size (how many sequences are processed together)\n",
        "* **T** â€” number of time steps\n",
        "* **F** â€” feature dimension of each time step\n",
        "\n",
        "During training, we go **through time step by step**:\n",
        "\n",
        "1. At **t = 1**, the network sees the first input `x[:, 0, :]`\n",
        "   â†’ It uses the initial memory **(hâ‚€, câ‚€)** (both zeros).\n",
        "2. The LSTM cell outputs new states **(hâ‚, câ‚)**.\n",
        "3. At **t = 2**, it takes the next input `x[:, 1, :]`\n",
        "   together with **(hâ‚, câ‚)**, producing **(hâ‚‚, câ‚‚)**.\n",
        "4. This repeats until **t = T**, where we obtain the final hidden state **h_T**.\n",
        "\n",
        "The key idea ðŸ’¡:\n",
        "\n",
        "> The hidden state `h` and cell state `c` act as the **memory** of the sequence,\n",
        "> carrying information from the past to influence future predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**In code:**\n",
        "\n",
        "```python\n",
        "# Go through each time step\n",
        "for t in range(T):\n",
        "    h, c = self.cell(x[:, t, :], h, c)\n",
        "```\n",
        "\n",
        "This loop means:\n",
        "\n",
        "* Each `x[:, t, :]` is one moment in time.\n",
        "* `h` and `c` are continuously updated.\n",
        "* The final `h` (after all time steps) summarizes the whole sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdec59a",
      "metadata": {
        "id": "3fdec59a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ManualLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Each gate has its own weight for input (W) and hidden (U)\n",
        "        self.W_i = nn.Linear(input_size, hidden_size)\n",
        "        self.U_i = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_f = nn.Linear(input_size, hidden_size)\n",
        "        self.U_f = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_o = nn.Linear(input_size, hidden_size)\n",
        "        self.U_o = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_c = nn.Linear(input_size, hidden_size)\n",
        "        self.U_c = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x_t, h_prev, c_prev):\n",
        "        # --- Compute gates ---\n",
        "        i_t = torch.sigmoid(self.W_i(x_t) + self.U_i(h_prev))   # input gate\n",
        "        f_t = torch.sigmoid(self.W_f(x_t) + self.U_f(h_prev))   # forget gate\n",
        "        o_t = torch.sigmoid(self.W_o(x_t) + self.U_o(h_prev))   # output gate\n",
        "        g_t = torch.tanh(self.W_c(x_t) + self.U_c(h_prev))      # candidate cell\n",
        "\n",
        "        # --- Update states ---\n",
        "        c_t = f_t * c_prev + i_t * g_t\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "        return h_t, c_t\n",
        "\n",
        "\n",
        "class SeqRegressor(nn.Module):\n",
        "    \"\"\"\n",
        "    Flexible sequence-to-value regression model supporting RNN/LSTM/GRU architectures.\n",
        "    \n",
        "    This model processes sequential input data and predicts a single continuous value.\n",
        "    It uses the final hidden state from the recurrent layers as the sequence representation.\n",
        "    \n",
        "    Architecture:\n",
        "        Input â†’ RNN/LSTM/GRU Layer(s) â†’ Dropout â†’ Linear â†’ Output\n",
        "    \n",
        "    Args:\n",
        "        input_size (int): Number of input features at each time step (default: 1 for univariate)\n",
        "        hidden_size (int): Dimension of hidden state in recurrent layers (default: 64)\n",
        "        num_layers (int): Number of stacked recurrent layers (default: 1)\n",
        "        cell_type (str): Type of recurrent cell - 'rnn', 'lstm', or 'gru' (default: 'lstm')\n",
        "        dropout (float): Dropout probability between recurrent layers (default: 0.2)\n",
        "                        Only applied when num_layers > 1\n",
        "        bidirectional (bool): Whether to use bidirectional recurrent layers (default: False)\n",
        "    \n",
        "    Shape:\n",
        "        Input: (batch_size, seq_len, input_size)\n",
        "        Output: (batch_size, 1)\n",
        "    \n",
        "    Example:\n",
        "        >>> model = SeqRegressor(input_size=1, hidden_size=128, num_layers=2, cell_type='lstm')\n",
        "        >>> x = torch.randn(32, 30, 1)  # batch=32, seq_len=30, features=1\n",
        "        >>> y = model(x)  # Output shape: (32, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        input_size: int = 1,\n",
        "        hidden_size: int = 64,\n",
        "        num_layers: int = 1,\n",
        "        cell_type: str = 'lstm',\n",
        "        dropout: float = 0.2,\n",
        "        bidirectional: bool = False\n",
        "    ):\n",
        "        super(SeqRegressor, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Validate cell type\n",
        "        valid_cells = {'rnn', 'lstm', 'gru'}\n",
        "        if self.cell_type not in valid_cells:\n",
        "            raise ValueError(f\"cell_type must be one of {valid_cells}, got '{self.cell_type}'\")\n",
        "        \n",
        "        # Select appropriate recurrent layer\n",
        "        if self.cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        elif self.cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        else:  # gru\n",
        "            self.rnn = nn.GRU(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        \n",
        "        # Additional dropout after recurrent layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Output layer\n",
        "        # If bidirectional, the final hidden size is doubled\n",
        "        fc_input_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        self.fc = nn.Linear(fc_input_size, 1)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights using Xavier/Glorot initialization for better gradient flow.\n",
        "        This helps prevent vanishing/exploding gradients in deep recurrent networks.\n",
        "        \"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                # Input-to-hidden weights\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'weight_hh' in name:\n",
        "                # Hidden-to-hidden weights (use orthogonal for RNNs)\n",
        "                nn.init.orthogonal_(param)\n",
        "            elif 'bias' in name:\n",
        "                # Initialize biases to zero\n",
        "                nn.init.zeros_(param)\n",
        "                # For LSTM, initialize forget gate bias to 1 (helps learning)\n",
        "                if self.cell_type == 'lstm':\n",
        "                    n = param.size(0)\n",
        "                    param.data[n//4:n//2].fill_(1.0)\n",
        "            elif 'fc' in name and 'weight' in name:\n",
        "                # Final linear layer\n",
        "                nn.init.xavier_uniform_(param)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the sequence regressor.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Output predictions of shape (batch_size, 1)\n",
        "        \n",
        "        Process:\n",
        "            1. Pass input through recurrent layers\n",
        "            2. Extract final hidden state\n",
        "            3. Apply dropout\n",
        "            4. Project to output dimension\n",
        "        \"\"\"\n",
        "        # x shape: (batch, seq_len, input_size)\n",
        "        \n",
        "        # Pass through recurrent layers\n",
        "        # output shape: (batch, seq_len, hidden_size * num_directions)\n",
        "        # h_n shape: (num_layers * num_directions, batch, hidden_size)\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (h_n, c_n) = self.rnn(x)\n",
        "        else:  # rnn or gru\n",
        "            output, h_n = self.rnn(x)\n",
        "        \n",
        "        # Extract final hidden state\n",
        "        # For bidirectional models, concatenate forward and backward final states\n",
        "        if self.bidirectional:\n",
        "            # h_n shape: (num_layers * 2, batch, hidden_size)\n",
        "            # Take last layer's forward and backward states\n",
        "            h_forward = h_n[-2, :, :]  # Forward direction of last layer\n",
        "            h_backward = h_n[-1, :, :]  # Backward direction of last layer\n",
        "            h_final = torch.cat([h_forward, h_backward], dim=1)\n",
        "        else:\n",
        "            # Take the last layer's hidden state\n",
        "            h_final = h_n[-1, :, :]  # Shape: (batch, hidden_size)\n",
        "        \n",
        "        # Apply dropout\n",
        "        h_final = self.dropout(h_final)\n",
        "        \n",
        "        # Project to output\n",
        "        out = self.fc(h_final)  # Shape: (batch, 1)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def get_model_info(self) -> dict:\n",
        "        \"\"\"\n",
        "        Get comprehensive model information for logging and debugging.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary containing model architecture details and parameter counts\n",
        "        \"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        \n",
        "        return {\n",
        "            'architecture': self.__class__.__name__,\n",
        "            'cell_type': self.cell_type.upper(),\n",
        "            'input_size': self.input_size,\n",
        "            'hidden_size': self.hidden_size,\n",
        "            'num_layers': self.num_layers,\n",
        "            'bidirectional': self.bidirectional,\n",
        "            'total_parameters': total_params,\n",
        "            'trainable_parameters': trainable_params,\n",
        "            'parameters_K': total_params / 1e3,\n",
        "            'parameters_M': total_params / 1e6\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Instantiate and Test Model\n",
        "# ============================================================================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create LSTM model with default configuration\n",
        "model = SeqRegressor(\n",
        "    input_size=1, \n",
        "    hidden_size=64, \n",
        "    num_layers=1, \n",
        "    cell_type='lstm',\n",
        "    dropout=0.2,\n",
        "    bidirectional=False\n",
        ").to(device)\n",
        "\n",
        "# Display model information\n",
        "model_info = model.get_model_info()\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Architecture:         {model_info['architecture']}\")\n",
        "print(f\"Cell Type:            {model_info['cell_type']}\")\n",
        "print(f\"Input Size:           {model_info['input_size']}\")\n",
        "print(f\"Hidden Size:          {model_info['hidden_size']}\")\n",
        "print(f\"Number of Layers:     {model_info['num_layers']}\")\n",
        "print(f\"Bidirectional:        {model_info['bidirectional']}\")\n",
        "print(f\"Total Parameters:     {model_info['total_parameters']:,}\")\n",
        "print(f\"Trainable Parameters: {model_info['trainable_parameters']:,}\")\n",
        "print(f\"Model Size:           {model_info['parameters_K']:.2f}K parameters\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nModel Structure:\")\n",
        "print(\"-\" * 70)\n",
        "print(model)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 30, 1).to(device)  # (batch=4, seq_len=30, features=1)\n",
        "test_output = model(test_input)\n",
        "print(f\"\\nTest Forward Pass:\")\n",
        "print(f\"Input shape:  {test_input.shape}\")\n",
        "print(f\"Output shape: {test_output.shape}\")\n",
        "print(f\"Output range: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02d32ce",
      "metadata": {
        "id": "c02d32ce"
      },
      "source": [
        "\n",
        "## 4) Train!\n",
        "\n",
        "We'll train with MSE loss and Adam. After training, we'll plot the loss curve and validation predictions.\n",
        "\n",
        "**This section is split into two cells** to keep code readable:\n",
        "- **4A** defines the training function.\n",
        "- **4B** runs training and draws the loss curves.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nKwz3j4Ez3MQ",
      "metadata": {
        "id": "nKwz3j4Ez3MQ"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ðŸ§© 1. The difference between **time steps (seq_len)** and **layers (num_layers)**\n",
        "\n",
        "| Concept                   | Meaning                                                                                                | Example                                                                  |\n",
        "| ------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n",
        "| **Time step (`seq_len`)** | The number of steps in the input sequence â€” how many moments the model processes sequentially.         | `seq_len = 30` means each input sequence has 30 consecutive data points. |\n",
        "| **Layer (`num_layers`)**  | The number of stacked RNN/LSTM layers â€” how many layers of hidden units are applied at each time step. | `num_layers = 1` means only one LSTM layer is used at each time step.    |\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ 2. How a single LSTM layer processes 30 time steps\n",
        "\n",
        "When you feed an input of shape `(batch=64, seq_len=30, features=1)` into the model,\n",
        "PyTorchâ€™s LSTM automatically iterates **30 times** over the time dimension.\n",
        "\n",
        "The logic is as follows:\n",
        "\n",
        "1ï¸âƒ£ **Step 1:** Input the first time point (x_1) with the initial hidden and cell states (h_0, c_0).\n",
        "â†’ Output new states (h_1, c_1).\n",
        "\n",
        "2ï¸âƒ£ **Step 2:** Input the next time point (x_2), together with (h_1, c_1).\n",
        "â†’ Output (h_2, c_2).\n",
        "\n",
        "3ï¸âƒ£ Continue this process until **step 30**, producing the final hidden state (h_{30}),\n",
        "which summarizes the entire input sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§± 3. What â€œone layerâ€ really means\n",
        "\n",
        "Setting `num_layers = 1` means that at each time step, the LSTM cell has only **one layer of computation**.\n",
        "That is:\n",
        "\n",
        "* Along the **time dimension**, the network still performs **30 sequential updates**.\n",
        "* Along the **depth (vertical stacking)** dimension, there is only one layer â€” no additional LSTMs stacked above it.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… In one sentence\n",
        "\n",
        "> Even with only one LSTM layer, the network still processes all 30 time steps in sequence.\n",
        "> **â€œnum_layers controls depth, seq_len controls length.â€**\n",
        "> A single-layer LSTM can read the entire 30-step sequence and update its memory at each step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "My1xcEWt0D0J",
      "metadata": {
        "id": "My1xcEWt0D0J"
      },
      "source": [
        "### ðŸ§  How a single-layer LSTM processes 30 time steps\n",
        "\n",
        "```\n",
        "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "       â”‚                    One training batch                      â”‚\n",
        "       â”‚        shape = (batch_size=64, seq_len=30, features=1)     â”‚\n",
        "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                            â”‚\n",
        "                            â–¼\n",
        "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "            â”‚        Single LSTM Layer       â”‚\n",
        "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                            â”‚\n",
        "                            â–¼\n",
        " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        " â”‚  xâ‚         â”‚  xâ‚‚         â”‚  xâ‚ƒ         â”‚ ...         â”‚  xâ‚ƒâ‚€        â”‚\n",
        " â”‚ (t=1)       â”‚ (t=2)       â”‚ (t=3)       â”‚             â”‚ (t=30)      â”‚\n",
        " â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        " â”‚  hâ‚€,câ‚€ â”€â”€â–¶ LSTM â”€â”€â–¶ hâ‚,câ‚ â”€â”€â–¶ LSTM â”€â”€â–¶ hâ‚‚,câ‚‚ â”€â”€â–¶ ... â”€â”€â–¶ hâ‚ƒâ‚€,câ‚ƒâ‚€ â”‚\n",
        " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                            â”‚\n",
        "                            â–¼\n",
        "              Take the last hidden state (hâ‚ƒâ‚€)\n",
        "                            â”‚\n",
        "                            â–¼\n",
        "                 Linear layer â†’ Predicted value (Å·)\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06c341d",
      "metadata": {
        "id": "f06c341d"
      },
      "outputs": [],
      "source": [
        "# 4A) Define training function\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    epochs: int = 15,\n",
        "    lr: float = 1e-3,\n",
        "    grad_clip: Optional[float] = None,\n",
        "    patience: int = 10,\n",
        "    verbose: bool = True\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "    \"\"\"\n",
        "    Train a sequence model with comprehensive logging and early stopping.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to train\n",
        "        train_loader: DataLoader for training data\n",
        "        val_loader: DataLoader for validation data\n",
        "        epochs: Maximum number of training epochs\n",
        "        lr: Learning rate for Adam optimizer\n",
        "        grad_clip: Maximum gradient norm (None to disable clipping)\n",
        "        patience: Number of epochs to wait for improvement before early stopping\n",
        "        verbose: Whether to print detailed training progress\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (train_losses, val_losses) lists\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    \n",
        "    train_losses, val_losses = [], []\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_state = None\n",
        "    epochs_without_improvement = 0\n",
        "    \n",
        "    # Training configuration display\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"TRAINING CONFIGURATION\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Optimizer:            Adam\")\n",
        "        print(f\"Learning Rate:        {lr}\")\n",
        "        print(f\"Loss Function:        MSE\")\n",
        "        print(f\"Gradient Clipping:    {grad_clip if grad_clip else 'Disabled'}\")\n",
        "        print(f\"Early Stopping:       Enabled (patience={patience})\")\n",
        "        print(f\"Max Epochs:           {epochs}\")\n",
        "        print(f\"Train Batches:        {len(train_loader)}\")\n",
        "        print(f\"Val Batches:          {len(val_loader)}\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\n{'Epoch':<8}{'Train Loss':<15}{'Val Loss':<15}{'Best':<8}{'Time(s)':<10}{'Status'}\")\n",
        "        print(\"-\" * 70)\n",
        "    \n",
        "    total_start_time = time.time()\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        \n",
        "        # ========== Training Phase ==========\n",
        "        model.train()\n",
        "        train_running_loss = 0.0\n",
        "        train_samples = 0\n",
        "        \n",
        "        for batch_idx, (xb, yb) in enumerate(train_loader):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(xb)\n",
        "            loss = loss_fn(predictions, yb)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping (prevents exploding gradients)\n",
        "            if grad_clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            train_running_loss += loss.item() * xb.size(0)\n",
        "            train_samples += xb.size(0)\n",
        "        \n",
        "        train_loss = train_running_loss / train_samples\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # ========== Validation Phase ==========\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_samples = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                predictions = model(xb)\n",
        "                loss = loss_fn(predictions, yb)\n",
        "                val_running_loss += loss.item() * xb.size(0)\n",
        "                val_samples += xb.size(0)\n",
        "        \n",
        "        val_loss = val_running_loss / val_samples\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        \n",
        "        # ========== Model Checkpoint & Early Stopping ==========\n",
        "        is_best = val_loss < best_val_loss\n",
        "        status = \"\"\n",
        "        \n",
        "        if is_best:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            epochs_without_improvement = 0\n",
        "            status = \"NEW BEST\"\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                status = \"EARLY STOP\"\n",
        "        \n",
        "        # Display progress\n",
        "        if verbose:\n",
        "            best_marker = \"*\" if is_best else \"\"\n",
        "            print(f\"{epoch:<8}{train_loss:<15.6f}{val_loss:<15.6f}{best_marker:<8}{epoch_time:<10.2f}{status}\")\n",
        "        \n",
        "        # Early stopping check\n",
        "        if epochs_without_improvement >= patience:\n",
        "            if verbose:\n",
        "                print(\"-\" * 70)\n",
        "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "                print(f\"Best validation loss: {best_val_loss:.6f} at epoch {best_epoch}\")\n",
        "            break\n",
        "    \n",
        "    total_time = time.time() - total_start_time\n",
        "    \n",
        "    # Load best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        if verbose:\n",
        "            print(\"-\" * 70)\n",
        "            print(f\"Loaded best model from epoch {best_epoch}\")\n",
        "    \n",
        "    # Training summary\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"TRAINING SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Total Training Time:  {total_time:.2f}s\")\n",
        "        print(f\"Epochs Completed:     {epoch}/{epochs}\")\n",
        "        print(f\"Best Epoch:           {best_epoch}\")\n",
        "        print(f\"Best Val Loss:        {best_val_loss:.6f}\")\n",
        "        print(f\"Final Train Loss:     {train_losses[-1]:.6f}\")\n",
        "        print(f\"Final Val Loss:       {val_losses[-1]:.6f}\")\n",
        "        print(f\"Improvement:          {(train_losses[0] - train_losses[-1]) / train_losses[0] * 100:.2f}%\")\n",
        "        print(\"=\" * 70)\n",
        "    \n",
        "    # ========== Visualization ==========\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "    \n",
        "    # Loss curves\n",
        "    ax1 = axes[0]\n",
        "    epochs_range = range(1, len(train_losses) + 1)\n",
        "    ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    ax1.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
        "    ax1.set_xlabel('Epoch', fontsize=11)\n",
        "    ax1.set_ylabel('MSE Loss', fontsize=11)\n",
        "    ax1.set_title('Training and Validation Loss Curves', fontsize=12, fontweight='bold')\n",
        "    ax1.legend(loc='best')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Loss reduction visualization\n",
        "    ax2 = axes[1]\n",
        "    train_reduction = [(train_losses[0] - loss) / train_losses[0] * 100 for loss in train_losses]\n",
        "    val_reduction = [(val_losses[0] - loss) / val_losses[0] * 100 for loss in val_losses]\n",
        "    ax2.plot(epochs_range, train_reduction, 'b-', label='Train Loss Reduction', linewidth=2)\n",
        "    ax2.plot(epochs_range, val_reduction, 'r-', label='Val Loss Reduction', linewidth=2)\n",
        "    ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "    ax2.set_xlabel('Epoch', fontsize=11)\n",
        "    ax2.set_ylabel('Loss Reduction (%)', fontsize=11)\n",
        "    ax2.set_title('Learning Progress (% Improvement)', fontsize=12, fontweight='bold')\n",
        "    ax2.legend(loc='best')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training & Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabc6ea3",
      "metadata": {
        "id": "eabc6ea3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4B) Run training\n",
        "_ = train_model(model, train_loader, val_loader, epochs=15, lr=1e-3, grad_clip=1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24df16d1",
      "metadata": {
        "id": "24df16d1"
      },
      "source": [
        "\n",
        "## 5) Inspect predictions vs. ground truth\n",
        "\n",
        "We take a few validation sequences and compare predicted next-step values with the true targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1464adb",
      "metadata": {
        "id": "c1464adb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Comprehensive Prediction Analysis\n",
        "==================================\n",
        "Evaluate model performance on validation data with detailed visualizations\n",
        "and statistical metrics.\n",
        "\"\"\"\n",
        "\n",
        "# ========== Generate Predictions ==========\n",
        "model.eval()\n",
        "xb, yb = next(iter(val_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(xb.to(device)).cpu().numpy().squeeze()\n",
        "\n",
        "ground_truth = yb.numpy().squeeze()\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(ground_truth, predictions)\n",
        "mae = mean_absolute_error(ground_truth, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(ground_truth, predictions)\n",
        "correlation = np.corrcoef(ground_truth, predictions)[0, 1]\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = ground_truth - predictions\n",
        "mean_residual = np.mean(residuals)\n",
        "std_residual = np.std(residuals)\n",
        "\n",
        "# ========== Display Metrics ==========\n",
        "print(\"=\" * 70)\n",
        "print(\"PREDICTION PERFORMANCE METRICS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Mean Squared Error (MSE):     {mse:.6f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE):    {mae:.6f}\")\n",
        "print(f\"RÂ² Score:                     {r2:.6f}\")\n",
        "print(f\"Correlation Coefficient:      {correlation:.6f}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Residual Mean:                {mean_residual:.6f}\")\n",
        "print(f\"Residual Std Dev:             {std_residual:.6f}\")\n",
        "print(f\"Max Error:                    {np.max(np.abs(residuals)):.6f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ========== Visualizations ==========\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Predictions vs Ground Truth (Time Series)\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "sample_indices = np.arange(len(ground_truth))\n",
        "ax1.plot(sample_indices, ground_truth, 'b-', label='Ground Truth', linewidth=2, alpha=0.7)\n",
        "ax1.plot(sample_indices, predictions, 'r--', label='Predictions', linewidth=2, alpha=0.7)\n",
        "ax1.fill_between(sample_indices, ground_truth, predictions, alpha=0.2, color='gray', label='Error')\n",
        "ax1.set_xlabel('Sample Index in Batch', fontsize=11)\n",
        "ax1.set_ylabel('Normalized Value', fontsize=11)\n",
        "ax1.set_title('Next-Step Predictions vs Ground Truth', fontsize=12, fontweight='bold')\n",
        "ax1.legend(loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Scatter Plot: Predictions vs Ground Truth\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "ax2.scatter(ground_truth, predictions, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
        "min_val = min(ground_truth.min(), predictions.min())\n",
        "max_val = max(ground_truth.max(), predictions.max())\n",
        "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "ax2.set_xlabel('Ground Truth', fontsize=11)\n",
        "ax2.set_ylabel('Predictions', fontsize=11)\n",
        "ax2.set_title(f'Scatter Plot (RÂ² = {r2:.4f})', fontsize=12, fontweight='bold')\n",
        "ax2.legend(loc='best')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axis('equal')\n",
        "\n",
        "# 3. Residual Plot\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "ax3.scatter(predictions, residuals, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
        "ax3.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Zero Residual')\n",
        "ax3.axhline(y=mean_residual, color='g', linestyle=':', linewidth=2, label=f'Mean ({mean_residual:.4f})')\n",
        "ax3.fill_between([predictions.min(), predictions.max()], \n",
        "                  -2*std_residual, 2*std_residual, \n",
        "                  alpha=0.2, color='yellow', label='Â±2Ïƒ')\n",
        "ax3.set_xlabel('Predicted Values', fontsize=11)\n",
        "ax3.set_ylabel('Residuals (Truth - Pred)', fontsize=11)\n",
        "ax3.set_title('Residual Analysis', fontsize=12, fontweight='bold')\n",
        "ax3.legend(loc='best')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Residual Distribution (Histogram)\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "ax4.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "ax4.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero')\n",
        "ax4.axvline(x=mean_residual, color='g', linestyle=':', linewidth=2, label=f'Mean ({mean_residual:.4f})')\n",
        "ax4.set_xlabel('Residual Value', fontsize=11)\n",
        "ax4.set_ylabel('Frequency', fontsize=11)\n",
        "ax4.set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
        "ax4.legend(loc='best')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 5. Error Distribution (Absolute Error)\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "abs_errors = np.abs(residuals)\n",
        "ax5.hist(abs_errors, bins=30, edgecolor='black', alpha=0.7, color='salmon')\n",
        "ax5.axvline(x=mae, color='b', linestyle='--', linewidth=2, label=f'MAE ({mae:.4f})')\n",
        "ax5.set_xlabel('Absolute Error', fontsize=11)\n",
        "ax5.set_ylabel('Frequency', fontsize=11)\n",
        "ax5.set_title('Absolute Error Distribution', fontsize=12, fontweight='bold')\n",
        "ax5.legend(loc='best')\n",
        "ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Comprehensive Prediction Analysis on Validation Batch', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.show()\n",
        "\n",
        "# ========== Sample-by-Sample Comparison (First 10) ==========\n",
        "print(\"\\nSample-by-Sample Comparison (First 10 samples):\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Index':<8}{'Ground Truth':<18}{'Prediction':<18}{'Error':<15}\")\n",
        "print(\"-\" * 70)\n",
        "for i in range(min(10, len(ground_truth))):\n",
        "    error = ground_truth[i] - predictions[i]\n",
        "    print(f\"{i:<8}{ground_truth[i]:<18.6f}{predictions[i]:<18.6f}{error:<15.6f}\")\n",
        "print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb08668",
      "metadata": {
        "id": "dcb08668"
      },
      "source": [
        "\n",
        "## 6) Comprehensive Exercises and Experiments\n",
        "\n",
        "### Exercise 1: Architecture Comparison (RNN vs LSTM vs GRU)\n",
        "\n",
        "**Objective:** Compare the performance of different recurrent architectures on the same task.\n",
        "\n",
        "**Tasks:**\n",
        "1. Train models with RNN, LSTM, and GRU cells (same hyperparameters)\n",
        "2. Compare final validation losses\n",
        "3. Analyze which architecture learns fastest\n",
        "4. Examine prediction quality differences\n",
        "\n",
        "**Expected Insights:**\n",
        "- LSTM/GRU typically outperform vanilla RNN on long sequences\n",
        "- LSTM has more parameters but better gradient flow\n",
        "- GRU is a good middle ground (fewer parameters, similar performance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981b5e23",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Exercise 1: RNN vs LSTM vs GRU Comparison\n",
        "\"\"\"\n",
        "\n",
        "# Configuration\n",
        "architectures = ['rnn', 'lstm', 'gru']\n",
        "results = {}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXERCISE 1: ARCHITECTURE COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for arch in architectures:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training {arch.upper()} Model\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create model\n",
        "    model_arch = SeqRegressor(\n",
        "        input_size=1,\n",
        "        hidden_size=64,\n",
        "        num_layers=1,\n",
        "        cell_type=arch,\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    \n",
        "    # Display model info\n",
        "    info = model_arch.get_model_info()\n",
        "    print(f\"Parameters: {info['total_parameters']:,} ({info['parameters_K']:.2f}K)\")\n",
        "    \n",
        "    # Train\n",
        "    train_losses, val_losses = train_model(\n",
        "        model_arch,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=20,\n",
        "        lr=1e-3,\n",
        "        grad_clip=1.0,\n",
        "        patience=7,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    results[arch] = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'final_train': train_losses[-1],\n",
        "        'final_val': val_losses[-1],\n",
        "        'best_val': min(val_losses),\n",
        "        'params': info['total_parameters']\n",
        "    }\n",
        "\n",
        "# ========== Comparison Visualization ==========\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Training loss comparison\n",
        "ax1 = axes[0]\n",
        "for arch in architectures:\n",
        "    ax1.plot(results[arch]['train_losses'], label=f'{arch.upper()}', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=11)\n",
        "ax1.set_ylabel('Training Loss', fontsize=11)\n",
        "ax1.set_title('Training Loss Comparison', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation loss comparison\n",
        "ax2 = axes[1]\n",
        "for arch in architectures:\n",
        "    ax2.plot(results[arch]['val_losses'], label=f'{arch.upper()}', linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=11)\n",
        "ax2.set_ylabel('Validation Loss', fontsize=11)\n",
        "ax2.set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========== Results Summary ==========\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ARCHITECTURE COMPARISON RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Architecture':<15}{'Parameters':<15}{'Best Val Loss':<18}{'Final Val Loss':<18}\")\n",
        "print(\"-\" * 70)\n",
        "for arch in architectures:\n",
        "    r = results[arch]\n",
        "    print(f\"{arch.upper():<15}{r['params']:<15,}{r['best_val']:<18.6f}{r['final_val']:<18.6f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find best architecture\n",
        "best_arch = min(results.items(), key=lambda x: x[1]['best_val'])\n",
        "print(f\"\\nBest Architecture: {best_arch[0].upper()} (Val Loss: {best_arch[1]['best_val']:.6f})\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce24155",
      "metadata": {},
      "source": [
        "\n",
        "### Exercise 2: Hidden Size Impact\n",
        "\n",
        "**Objective:** Understand how hidden layer size affects model capacity and performance.\n",
        "\n",
        "**Tasks:**\n",
        "1. Train models with hidden sizes: [32, 64, 128, 256]\n",
        "2. Track parameter count vs performance\n",
        "3. Identify the optimal hidden size\n",
        "4. Check for underfitting vs overfitting\n",
        "\n",
        "**Expected Insights:**\n",
        "- Larger hidden sizes = more capacity but more parameters\n",
        "- Too small â†’ underfitting, too large â†’ overfitting/slower training\n",
        "- Optimal size depends on data complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7094859",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Exercise 2: Hidden Size Experiment\n",
        "\"\"\"\n",
        "\n",
        "hidden_sizes = [32, 64, 128, 256]\n",
        "hidden_results = {}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXERCISE 2: HIDDEN SIZE IMPACT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for h_size in hidden_sizes:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training with Hidden Size = {h_size}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    model_h = SeqRegressor(\n",
        "        input_size=1,\n",
        "        hidden_size=h_size,\n",
        "        num_layers=1,\n",
        "        cell_type='lstm',\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    \n",
        "    info = model_h.get_model_info()\n",
        "    print(f\"Parameters: {info['total_parameters']:,} ({info['parameters_K']:.2f}K)\")\n",
        "    \n",
        "    train_losses, val_losses = train_model(\n",
        "        model_h,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=20,\n",
        "        lr=1e-3,\n",
        "        grad_clip=1.0,\n",
        "        patience=7,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    hidden_results[h_size] = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'params': info['total_parameters'],\n",
        "        'best_val': min(val_losses),\n",
        "        'final_val': val_losses[-1],\n",
        "        'overfitting_gap': train_losses[-1] - val_losses[-1]\n",
        "    }\n",
        "\n",
        "# ========== Visualization ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Loss curves\n",
        "ax1 = axes[0, 0]\n",
        "for h_size in hidden_sizes:\n",
        "    ax1.plot(hidden_results[h_size]['val_losses'], label=f'H={h_size}', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Validation Loss')\n",
        "ax1.set_title('Validation Loss vs Hidden Size', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Parameters vs Performance\n",
        "ax2 = axes[0, 1]\n",
        "params = [hidden_results[h]['params'] for h in hidden_sizes]\n",
        "best_vals = [hidden_results[h]['best_val'] for h in hidden_sizes]\n",
        "ax2.plot(params, best_vals, 'bo-', linewidth=2, markersize=8)\n",
        "for h in hidden_sizes:\n",
        "    ax2.annotate(f'H={h}', (hidden_results[h]['params'], hidden_results[h]['best_val']),\n",
        "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "ax2.set_xlabel('Number of Parameters')\n",
        "ax2.set_ylabel('Best Validation Loss')\n",
        "ax2.set_title('Model Capacity vs Performance', fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Overfitting analysis\n",
        "ax3 = axes[1, 0]\n",
        "gaps = [hidden_results[h]['overfitting_gap'] for h in hidden_sizes]\n",
        "ax3.bar([str(h) for h in hidden_sizes], gaps, color='coral', edgecolor='black')\n",
        "ax3.set_xlabel('Hidden Size')\n",
        "ax3.set_ylabel('Train-Val Gap')\n",
        "ax3.set_title('Overfitting Analysis (Train - Val Loss)', fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Summary table visualization\n",
        "ax4 = axes[1, 1]\n",
        "ax4.axis('off')\n",
        "table_data = []\n",
        "for h in hidden_sizes:\n",
        "    r = hidden_results[h]\n",
        "    table_data.append([\n",
        "        f'{h}',\n",
        "        f'{r[\"params\"]:,}',\n",
        "        f'{r[\"best_val\"]:.6f}',\n",
        "        f'{r[\"overfitting_gap\"]:.6f}'\n",
        "    ])\n",
        "\n",
        "table = ax4.table(cellText=table_data,\n",
        "                 colLabels=['Hidden\\nSize', 'Parameters', 'Best Val\\nLoss', 'Overfit\\nGap'],\n",
        "                 cellLoc='center',\n",
        "                 loc='center',\n",
        "                 colWidths=[0.15, 0.25, 0.25, 0.25])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "ax4.set_title('Performance Summary', fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Best hidden size\n",
        "best_h = min(hidden_results.items(), key=lambda x: x[1]['best_val'])\n",
        "print(f\"\\nOptimal Hidden Size: {best_h[0]} (Val Loss: {best_h[1]['best_val']:.6f})\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9062f95f",
      "metadata": {},
      "source": [
        "\n",
        "### Exercise 3: Sequence Length Impact\n",
        "\n",
        "**Objective:** Investigate how input sequence length affects prediction accuracy.\n",
        "\n",
        "**Tasks:**\n",
        "1. Train models with different sequence lengths: [10, 20, 30, 50]\n",
        "2. Analyze how longer context improves/hurts predictions\n",
        "3. Examine training efficiency vs sequence length\n",
        "\n",
        "**Expected Insights:**\n",
        "- Longer sequences provide more context but slower training\n",
        "- Very short sequences may lack sufficient information\n",
        "- Optimal length depends on temporal dependencies in data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222671b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Exercise 3: Sequence Length Experiment\n",
        "\"\"\"\n",
        "\n",
        "seq_lengths = [10, 20, 30, 50]\n",
        "seq_results = {}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXERCISE 3: SEQUENCE LENGTH IMPACT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training with Sequence Length = {seq_len}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create datasets with specific sequence length\n",
        "    train_ds_seq = SineMixDataset(train=True, seq_len=seq_len)\n",
        "    val_ds_seq = SineMixDataset(train=False, seq_len=seq_len)\n",
        "    \n",
        "    train_loader_seq = DataLoader(train_ds_seq, batch_size=64, shuffle=True, drop_last=True)\n",
        "    val_loader_seq = DataLoader(val_ds_seq, batch_size=64, shuffle=False, drop_last=False)\n",
        "    \n",
        "    print(f\"Train samples: {len(train_ds_seq)}, Val samples: {len(val_ds_seq)}\")\n",
        "    \n",
        "    # Create and train model\n",
        "    model_seq = SeqRegressor(\n",
        "        input_size=1,\n",
        "        hidden_size=64,\n",
        "        num_layers=1,\n",
        "        cell_type='lstm',\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    \n",
        "    train_losses, val_losses = train_model(\n",
        "        model_seq,\n",
        "        train_loader_seq,\n",
        "        val_loader_seq,\n",
        "        epochs=20,\n",
        "        lr=1e-3,\n",
        "        grad_clip=1.0,\n",
        "        patience=7,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    seq_results[seq_len] = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_val': min(val_losses),\n",
        "        'final_val': val_losses[-1],\n",
        "        'n_samples': len(train_ds_seq)\n",
        "    }\n",
        "\n",
        "# ========== Visualization ==========\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss comparison\n",
        "ax1 = axes[0]\n",
        "for seq_len in seq_lengths:\n",
        "    ax1.plot(seq_results[seq_len]['val_losses'], label=f'SeqLen={seq_len}', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Validation Loss')\n",
        "ax1.set_title('Validation Loss vs Sequence Length', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Best performance vs sequence length\n",
        "ax2 = axes[1]\n",
        "best_vals = [seq_results[sl]['best_val'] for sl in seq_lengths]\n",
        "ax2.plot(seq_lengths, best_vals, 'ro-', linewidth=2, markersize=10)\n",
        "for sl in seq_lengths:\n",
        "    ax2.annotate(f'{seq_results[sl][\"best_val\"]:.5f}',\n",
        "                (sl, seq_results[sl]['best_val']),\n",
        "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9)\n",
        "ax2.set_xlabel('Sequence Length')\n",
        "ax2.set_ylabel('Best Validation Loss')\n",
        "ax2.set_title('Optimal Sequence Length Analysis', fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SEQUENCE LENGTH COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Seq Length':<15}{'Train Samples':<18}{'Best Val Loss':<20}{'Final Val Loss'}\")\n",
        "print(\"-\" * 70)\n",
        "for sl in seq_lengths:\n",
        "    r = seq_results[sl]\n",
        "    print(f\"{sl:<15}{r['n_samples']:<18}{r['best_val']:<20.6f}{r['final_val']:.6f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_seq = min(seq_results.items(), key=lambda x: x[1]['best_val'])\n",
        "print(f\"\\nOptimal Sequence Length: {best_seq[0]} (Val Loss: {best_seq[1]['best_val']:.6f})\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d3b4fc",
      "metadata": {},
      "source": [
        "\n",
        "## 7) Conclusion and Key Takeaways\n",
        "\n",
        "### What You've Learned\n",
        "\n",
        "1. **Sequence Data Fundamentals**\n",
        "   - How to structure time series data for RNN training\n",
        "   - Importance of sliding windows and proper train/val splits\n",
        "   - Shape conventions: (batch, seq_len, features)\n",
        "\n",
        "2. **Recurrent Architectures**\n",
        "   - Implemented and compared RNN, LSTM, and GRU\n",
        "   - Understanding of hidden states and temporal dependencies\n",
        "   - How recurrent layers process sequences step-by-step\n",
        "\n",
        "3. **Training Best Practices**\n",
        "   - Gradient clipping prevents exploding gradients\n",
        "   - Early stopping prevents overfitting\n",
        "   - Proper validation monitoring is crucial\n",
        "\n",
        "4. **Hyperparameter Impact**\n",
        "   - Architecture choice (LSTM usually best for long dependencies)\n",
        "   - Hidden size balances capacity vs efficiency\n",
        "   - Sequence length affects both performance and computation\n",
        "\n",
        "### Professional Development Skills Gained\n",
        "\n",
        "- Type hints and comprehensive docstrings\n",
        "- Structured logging and progress tracking\n",
        "- Professional visualization techniques\n",
        "- Modular, reusable code design\n",
        "- Comprehensive error analysis\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Apply to real-world time series (stock prices, weather, sensor data)\n",
        "- Explore attention mechanisms and Transformers\n",
        "- Try bidirectional models for non-causal tasks\n",
        "- Implement multi-step ahead prediction\n",
        "- Add more sophisticated regularization techniques\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
