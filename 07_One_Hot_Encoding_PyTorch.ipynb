{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d807aa1",
   "metadata": {},
   "source": [
    "# Understanding One-Hot Encoding in PyTorch\n",
    "\n",
    "This notebook focuses **only** on understanding and practicing *one-hot encoding*.\n",
    "\n",
    "By the end, you will understand:\n",
    "- What one-hot encoding means\n",
    "- Why we use it in classification and recognition\n",
    "- How to implement it in **PyTorch** step-by-step\n",
    "- When you do *not* need to use it manually (e.g., with `CrossEntropyLoss`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8d626",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What is One-Hot Encoding?\n",
    "In classification tasks, we often have multiple classes, e.g., Cat, Dog, Rabbit.\n",
    "We need a way to tell the network which class is the correct one.\n",
    "\n",
    "üëâ One-hot encoding represents the label as a **vector of 0s and 1s**, where only the correct class is 1.\n",
    "\n",
    "### Example (3 classes)\n",
    "| Class | One-hot Vector |\n",
    "|:------|:---------------|\n",
    "| Cat   | [1, 0, 0] |\n",
    "| Dog   | [0, 1, 0] |\n",
    "| Rabbit| [0, 0, 1] |\n",
    "\n",
    "This is how the model knows *which* class is correct among many options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee6d3a",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Why Do We Use One-Hot Encoding?\n",
    "**1. Clarity:** Only one class is marked as correct (1), others are 0.\n",
    "\n",
    "**2. Compatibility:** Softmax outputs a probability vector of the same size ‚Äî easy to compare.\n",
    "\n",
    "**3. Independence:** Class numbers (0,1,2,3...) have no real distance meaning. One-hot avoids confusion like 'class 2 is closer to class 1.'\n",
    "\n",
    "üìå *Summary:* One-hot encoding clearly indicates which class is correct and matches the output format of classification networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ea114",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Creating One-Hot Encodings in PyTorch\n",
    "PyTorch provides a built-in function: `torch.nn.functional.one_hot()`\n",
    "\n",
    "### Example: single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6893fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Suppose we have 4 possible classes (0,1,2,3)\n",
    "label = torch.tensor([2])  # class index 2\n",
    "one_hot = F.one_hot(label, num_classes=4)\n",
    "print('Label index:', label)\n",
    "print('One-hot vector:', one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5afd2",
   "metadata": {},
   "source": [
    "### Example: batch of labels\n",
    "Let's encode multiple samples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([0, 1, 2, 3])  # batch of labels\n",
    "one_hot_batch = F.one_hot(labels, num_classes=4)\n",
    "print('Labels:', labels)\n",
    "print('One-hot batch:\\n', one_hot_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be7096",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Visualizing One-Hot Vectors\n",
    "Let‚Äôs visualize how a one-hot vector looks like for a batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.imshow(one_hot_batch.numpy(), cmap='Greens')\n",
    "plt.title('Visualization of One-Hot Encodings (Batch)')\n",
    "plt.xlabel('Class index')\n",
    "plt.ylabel('Sample index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ad447",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ When You Don't Need One-Hot Encoding\n",
    "In **PyTorch**, some functions (like `nn.CrossEntropyLoss`) do **not** require one-hot labels.\n",
    "\n",
    "- `CrossEntropyLoss` expects **class indices**, not one-hot vectors.\n",
    "- Internally, it converts them to one-hot style automatically to compute the loss.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb17bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "outputs = torch.tensor([[2.0, 0.5, 0.3]])  # logits for 3 classes\n",
    "labels = torch.tensor([0])  # correct class index\n",
    "loss = criterion(outputs, labels)\n",
    "print('Loss (no one-hot needed):', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e9633",
   "metadata": {},
   "source": [
    "## üß© Mini Exercise\n",
    "Try changing the label values below and observe the new one-hot vectors!\n",
    "\n",
    "**Task:** Modify the labels tensor and rerun the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different label combinations\n",
    "labels = torch.tensor([1, 3, 0, 2])\n",
    "one_hot_batch = F.one_hot(labels, num_classes=4)\n",
    "print('Labels:', labels)\n",
    "print('One-hot vectors:\\n', one_hot_batch)\n",
    "\n",
    "plt.imshow(one_hot_batch.numpy(), cmap='Greens')\n",
    "plt.title('Updated One-Hot Encodings')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
